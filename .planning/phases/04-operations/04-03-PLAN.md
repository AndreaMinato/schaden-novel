---
phase: 04-operations
plan: 03
type: execute
wave: 2
depends_on:
  - 04-02
files_modified:
  - scripts/strip-dump-bodies.mjs
autonomous: false
requirements:
  - OPS-03

must_haves:
  truths:
    - "nuxt generate completes successfully with all 13K chapters"
    - "SQL dump files have chapter bodies stripped (90%+ size reduction)"
    - "Pre-rendered chapter pages still contain full body text"
  artifacts:
    - path: "scripts/strip-dump-bodies.mjs"
      provides: "Post-build SQL dump body stripper"
      contains: "__nuxt_content"
    - path: ".output/public/"
      provides: "Complete static site with all 13K chapters"
  key_links:
    - from: "scripts/strip-dump-bodies.mjs"
      to: ".output/public/__nuxt_content/"
      via: "reads and rewrites sql_dump files"
      pattern: "sql_dump"
---

<objective>
Create SQL dump body-stripping script, run full site build with 13K chapters, and verify the complete site works.

Purpose: OPS-03 final validation. Proves all 13,318 chapters build and are accessible. Body stripping keeps client-side SQLite dumps under localStorage limits.
Output: scripts/strip-dump-bodies.mjs, verified .output/public/ with full site
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-operations/04-RESEARCH.md
@.planning/phases/04-operations/04-02-SUMMARY.md
@nuxt.config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create body-stripping script and run full build</name>
  <files>scripts/strip-dump-bodies.mjs</files>
  <action>
**Step 1: Create scripts/strip-dump-bodies.mjs**

Post-processes SQL dump files AFTER `nuxt generate` to strip chapter body content from the client-side SQLite dumps. Pre-rendered HTML pages already contain the full body, so stripping from the dump only affects client-side navigation queries (which only need metadata like title, path, stem).

CRITICAL: Do NOT use `content:file:afterParse` hook — it strips body from the SQLite DB used for BOTH dumps AND pre-rendering, breaking pre-rendered pages (see research Pitfall 1).

Implementation approach:
```javascript
import { readFileSync, writeFileSync, readdirSync, existsSync } from 'fs'
import { gunzipSync, gzipSync } from 'zlib'
import { resolve } from 'path'

const dumpDir = resolve('.output/public/__nuxt_content')

if (!existsSync(dumpDir)) {
  console.error('No __nuxt_content directory found. Run nuxt generate first.')
  process.exit(1)
}

const collections = readdirSync(dumpDir, { withFileTypes: true })
  .filter(d => d.isDirectory())
  .map(d => d.name)

let totalBefore = 0, totalAfter = 0

for (const collection of collections) {
  const dumpPath = resolve(dumpDir, collection, 'sql_dump.txt')
  if (!existsSync(dumpPath)) continue

  const raw = readFileSync(dumpPath, 'utf-8')
  const before = Buffer.byteLength(raw, 'utf-8')
  totalBefore += before

  // Decode base64 -> decompress gzip -> get SQL text
  const buffer = Buffer.from(raw, 'base64')
  const sql = gunzipSync(buffer).toString('utf-8')

  // Strip body column from INSERT statements
  // Body is JSON minimark: {"type":"minimark","value":[...]}
  // Replace with empty minimark to preserve column structure
  const stripped = sql.replace(
    /'\{"type":"minimark","value":\[.*?\]\}'/g,
    '\'{"type":"minimark","value":[]}\''
  )

  // Recompress and re-encode
  const recompressed = gzipSync(Buffer.from(stripped))
  const output = recompressed.toString('base64')
  writeFileSync(dumpPath, output)

  const after = Buffer.byteLength(output, 'utf-8')
  totalAfter += after
  console.log(`${collection}: ${(before/1024).toFixed(0)}KB -> ${(after/1024).toFixed(0)}KB (${((1 - after/before) * 100).toFixed(0)}% reduction)`)
}

console.log(`\nTotal: ${(totalBefore/1024/1024).toFixed(1)}MB -> ${(totalAfter/1024/1024).toFixed(1)}MB`)
```

The regex approach may not handle all edge cases (escaped quotes in body content). If it fails, fall back to a line-by-line SQL parsing approach that identifies INSERT VALUE tuples and replaces the body column positionally. Test by checking that the output is still valid base64 that decompresses to valid SQL.

**Step 2: Kill any zombie nuxt processes, then run full build**
```bash
pkill -9 -f nuxt 2>/dev/null || true
NODE_OPTIONS=--max-old-space-size=8192 npx nuxt generate
```
This will take a significant time (~50 min estimated for 13K chapters based on linear extrapolation from 40s for 2,419 chapters). Wait for completion.

**Step 3: Run body stripping**
```bash
node scripts/strip-dump-bodies.mjs
```
Expect ~90%+ size reduction per collection. Total should drop from ~45MB to ~3-6MB.

**Step 4: Verify dump integrity**
Spot-check a stripped dump by decoding:
```bash
node -e "
const fs = require('fs');
const zlib = require('zlib');
const raw = fs.readFileSync('.output/public/__nuxt_content/mga/sql_dump.txt', 'utf-8');
const sql = zlib.gunzipSync(Buffer.from(raw, 'base64')).toString('utf-8');
console.log(sql.substring(0, 500));
console.log('...');
console.log('Total length:', sql.length);
"
```
SQL should be valid and body columns should be empty minimark.

**Step 5: Verify pre-rendered pages still have body**
```bash
# Check a random chapter HTML file has content (not empty body)
grep -l 'Capitolo' .output/public/novels/mga/1.html | head -1
# Check file size is reasonable (not just metadata)
wc -c .output/public/novels/mga/1.html
```
  </action>
  <verify>
`ls .output/public/__nuxt_content/` shows directories for all 10 novels.
`node scripts/strip-dump-bodies.mjs` runs without errors and reports size reductions.
`find .output/public/novels -name '*.html' | wc -l` shows all chapter pages generated.
Pre-rendered HTML files contain chapter body text (grep for 'Capitolo' in sample page).
  </verify>
  <done>nuxt generate completed with all 13K chapters, SQL dumps stripped of bodies, pre-rendered pages retain full content.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify complete site in browser</name>
  <files>none (verification only)</files>
  <action>
Start local preview server and verify the complete site works end-to-end.
Run: `npx serve .output/public`

What was built: Complete 13K-chapter static site with body-stripped SQL dumps. All 10 novels migrated, import scripts ported, legacy directories deleted.

Verification steps for human:
1. Visit http://localhost:3000 — home page should show all 10 novels with latest chapters
2. Visit http://localhost:3000/novels — catalog should list all 10 novels with chapter counts
3. Visit http://localhost:3000/novels/atg/1 — chapter should display with full body text
4. Visit http://localhost:3000/novels/mga/2335 — last mga chapter should work
5. Navigate prev/next — should work across chapters
6. Visit http://localhost:3000/rss.xml — should be valid RSS
7. Visit http://localhost:3000/novels/atg/rss.xml — per-novel RSS should work
8. Check browser console for QuotaExceededError — should have none (dumps are small)
  </action>
  <verify>Human confirms all verification steps pass by typing "approved".</verify>
  <done>User has verified the complete site works end-to-end with all 13K chapters.</done>
</task>

</tasks>

<verification>
1. `nuxt generate` exits 0
2. `find .output/public/novels -name '*.html' | wc -l` ~ 13,318+ (chapters + index pages)
3. Body stripping reduces total dump size from ~45MB to ~3-6MB
4. Pre-rendered HTML files contain chapter body text
5. Local preview serves all pages correctly
6. No QuotaExceededError in browser console
</verification>

<success_criteria>
- Full site builds with all 13,318 chapters without errors
- SQL dumps are body-stripped (90%+ reduction, under localStorage limit)
- Pre-rendered chapter pages display full body text
- User confirms site works end-to-end via local preview
</success_criteria>

<output>
After completion, create `.planning/phases/04-operations/04-03-SUMMARY.md`
</output>
